\subsubsection*{8) Parallelization \& OpenMP}

\textbf{Parallelization Steps:}
1) \textbf{Decomposition:} Split app into tasks/data blocks. Types: \textbf{data, functional, recursive, exploratory, speculatory}. Factors: app type, concurrency degree (embarrassingly parallel/serial or pleasingly parallel), granularity (fine/medium/coarse), target system.  
2) \textbf{Dependency Analysis:} Control (order), data (flow, anti, output).  
3) \textbf{Mapping:} Assign tasks to processors. Static = compile-time, dynamic = runtime.  
4) \textbf{Programming:} Express parallelism via API (e.g., OpenMP, MPI).

\textbf{Task Graph:} DAG where nodes = tasks, edges = dependencies/comm. costs. Used for static scheduling.

\textbf{Communication:} Locality matters! Intra-node = fast, inter-node = slow. Balance load to avoid bottlenecks. False sharing \& cache effects impact performance.

\textbf{OpenMP:} Shared-memory API. Use \#pragma omp for loops, parallel regions. Threads share vars; race conditions solved with \textbf{synchronization} (critical, barriers).  
Clauses: \textbf{private}, \textbf{shared}, \textbf{reduction}.  
Functions: \texttt{omp\_get\_thread\_num()}, \texttt{omp\_get\_num\_procs()}, \texttt{omp\_in\_parallel()}, etc.  
Env vars: \texttt{OMP\_NUM\_THREADS}, \texttt{OMP\_SCHEDULE}.  
Implicit barriers at end of parallel for; \texttt{nowait} disables them.  
Targets data parallelism, loops.

\textbf{Limits:} Shared-memory only, limited scalability, Amdahlâ€™s Law: $S = 1/(f + (1-f)/N)$. Need to expose enough parallelism, ensure load balance, optimize data locality.